# Pill Metrics Learning with Multihead Attention
In object recognition, especially, when new classes can easily appear during the application, few-shot learning has 
great importance. Metrics learning is an important elementary technique for few-shot object recognition which can be 
applied successfully for pill recognition. To enforce the exploitation of different object features we use 
multi-stream metrics learning networks for pill recognition in our article. We investigate the usage of 
multihead attention layers at different parts of the network. The performance is analyzed on two datasets with 
superior results to a state-of-the-art multi-stream pill recognition network.

## Architecture of the model

The main idea behind multi-stream processing is to persuade the sub-networks to focus on different kinds of features. For this reason different pre-processing steps are done in the streams:

-  **RGB**: color images are directly fed to a CNN for metrics embedding. We evaluated both EfficientNet-B0 [2] and 
EfficientNetv2 S [4]. EfficientNet-B0 has significantly fewer parameters than the CNN of [1] 
(5.3 million vs. 9 million) and it is well-optimized for similar tasks. EfficientNetv2 is larger (21.4M parameters) 
but is reported to be more accurate in ImageNet tasks and faster in train time. The same networks were used in all 
streams but with smaller number of parameters due to their grayscale input images.
-  **Contour**: images are generated by running the Canny edge detector on smoothed grayscale version of images 
(applying a 7×7 Gaussian kernel).
-  **Texture**: images are created by subtracting the smoothed and grayscale versions of pill images.
-  **Local Binary Patterns (LBP)**: LBP is a popular handcrafted local descriptor for many computer vision tasks 
including handwritten or printed OCR. That is the reason why we omitted the special OCR stream of [1] but computed the 
LBP images of the grayscale inputs and used them in similar streams as the others.

<figure align="center">
  <figcaption>Phase 1</figcaption>
  <img src="poc_images/substreams.png" alt="phase_1" width="1000"/>
</figure>

All streams received the bounding box defined pill images of resolution 224×224 detected by YOLOv7 as described above.
Before the concatenation of the embedding vectors we implemented the attention encoder in each stream. 
To fuse the information of the streams we concatenated the output vectors and applied full connections in one hidden 
and one output layer to generate the final embedding. During the training of the fusion network streams were frozen 
and only the top layers were trained.

<figure align="center">
  <figcaption>Phase 2</figcaption>
  <img src="poc_images/overview.png" alt="phase_2" width="1000"/>
</figure>


The following configurations were evaluated:

-  **EffNetV1+SA**: EfficientNet-B0 and separated self-attention. 
-  **EffNetV2+SA**: EfficientNetV2 S and separated self-attention.
-  **EffNetV2+MHA**: EfficientNetV2 S and separated multihead attention.
-  **EffNetV2+MHA+FMHA**: EfficientNetV2 S, separated multihead attention and multihead attention in the fusion network.
-  **EffNetV2+MHA+FMHA+BA**: EfficientNetV2 S, separated multihead attention, multihead attention in the fusion network, and batch all (BA) strategy for the fusion network.




## Datasets
We used two datasets, namely CURE [1] and our novel, custom-made one, entitled OGYEIv1 [3]. 
CURE is available online via this link:

https://drive.google.com/drive/folders/1dcqUaTSepplc4GAUC05mr9iReWVqaThN.

Ours can be accessed if you contact me via my e-mail address: radli.richard@mik.uni-pannon.hu

The comparison of the two datasets can be seen in the table below:

|                        | CURE                | OGYEIv1   |
|------------------------|---------------------|-----------|
| Number of pill classes | 196                 | 78        |
| Number of images       | 8973                | 3154      |
| Image resolution       | 800×800 - 2448×2448 | 2465×1683 |
| Instance per class     | 40-50               | 40-60     |
| Segmentation labels    | no                  | fully     |
| Backgrounds            | 6                   | 1         | 
| Imprinted text labels  | yes                 | yes       |

## Requirement
Make sure you have the following dependencies installed:

```bash
colorama>=0.4.6
colorlog>=6.7.0
json>=2.0.9
matplotlib>=3.7.1
numpy>=1.23.5
opencv-python>=4.5.5.64
pandas>=2.0.0
Pillow>=9.3.0
seaborn>=0.12.2
segmentation_models_pytorch>=0.3.3
skimage>=0.20.0
sklearn>=1.2.2
tkinter>=8.6.12
torch>=2.0.0+cu117
torchsummary>=1.5.1
torchvision>=0.15.1+cu117
tqdm>=4.65.0
```

## Installation
First, clone/download this repository. In the const.py file you will find this:

```python
root_mapping = {
    'ricsi': {
        "PROJECT_ROOT": 'D:/pill_detect/storage/',
        "DATASET_ROOT": 'D:/pill_detect/datasets'
    }
}
```

- Update the designated username ('ricsi') to reflect the username associated with your logged-in operating system.
- Utilize PROJECT_ROOT as the central repository for storing essential data.
- Employ DATASET_ROOT as the designated directory for managing datasets integral to the functioning of the project.
- const.py will create all the necessary folders.
- Download the datasets and place them into the appropriate folders.


## Overview of the repository
In the config.py file, key parameters and settings crucial for the training, testing, and data augmentation processes 
are centrally stored. These configurations provide a streamlined and organized approach to manage various aspects 
of the project, ensuring adaptability and ease of customization.

To create a dataset, we attached a couple of scripts for image capturing. Steps to create a dataset:

- Use `take_calibration_images.py`, and capture images for camera calibration. The more images are taken, 
better the results will be.
- Use `camera_calibration.py` to calibrate the camera.
- Use `camera_recording.py`, and create the image dataset.
- Finally, use `undistort_images.py` to undistort the taken images.

A couple of useful scripts are also part of this repository, these files can be found in the **dataset_operations** folder.
These tools are handful, if you wish to make operations on the datasets, such as splitting, checking the balance and
annotations, etc. Also, it is worth pointing out to the `utils.py` script, that has many useful functions.

## Usage

If the repository is cloned/downloaded, the root paths are sorted out, the datasets are in place, and everything is 
set up in the config files, the next step is to apply a trained YOLOv7 network for pill detection and use 
`crop_yolo_detected_images.py` on the detected images. For more details read our article [3]. 

Alternatively, if you can use `draw_masks.py` to create the binary mask images.

To create the images for the streams, run `create_stream_images.py`. Make sure you go through all the choices of 
the argument called **dataset_operation** in the **ConfigStreamNetwork** class in the `config.py` file (train, valid, test).

Next step is to train the stream networks, this is Phase 1. 
There are two kind of backbones are available for this, EfficientNet V1 b0 [2] and EfficientNet V2 s [4]. 
Make sure you train all four streams. Also, two loss functions are provided:
triplet loss and hard mining triplet loss. The later will save the hardest triplets, which can be utilized in Phase 2.
To copy the hardest samples into a directory, run `mine_hard_samples.py`.

After the streams are trained, the last step is to train the fusion network, it is also called Phase 2.
There are 5 choices for this, as listed above. If hard mining triplet loss was selected in Phase 1, 
the network will be trained on only the hardest samples.

To evaluate the models, use `predict_fusion_network.py`.

## References
[1] - Ling, S., Pastor, A., Li, J., Che, Z., Wang, J., Kim, J., & Callet, P. L. (2020). Few-shot pill recognition. 
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 9789-9798).

[2] - Tan, M., & Le, Q. (2019, May). Efficientnet: Rethinking model scaling for convolutional neural networks. 
In International conference on machine learning (pp. 6105-6114). PMLR.

[3] - Rádli, R.; Vörösházi, Z. and Czúni, L. (2023). Pill Metrics Learning with Multihead Attention.  
In Proceedings of the 15th International Joint Conference on Knowledge Discovery, Knowledge Engineering and
Knowledge Management - Volume 1: KDIR, ISBN 978-989-758-671-2, ISSN 2184-3228, pages 132-140.    

[4] - Tan, M., & Le, Q. (2021, July). Efficientnetv2: Smaller models and faster training. 
In International conference on machine learning (pp. 10096-10106). PMLR.