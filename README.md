![Python](https://img.shields.io/badge/python-v3.11-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)
![PyTorch](https://img.shields.io/badge/PyTorch-v2.2.1-%23EE4C2C.svg?style=for-the-badge&logo=PyTorch&logoColor=white)
![scikit-learn](https://img.shields.io/badge/scikit-v1.4.0--learn-%23F7931E.svg?style=for-the-badge&logo=scikit-learn&logoColor=white)
![NumPy](https://img.shields.io/badge/numpy-v1.26-%23013243.svg?style=for-the-badge&logo=numpy&logoColor=white)
![Pandas](https://img.shields.io/badge/pandas-v2.1.0-%23150458.svg?style=for-the-badge&logo=pandas&logoColor=white)
![Matplotlib](https://img.shields.io/badge/Matplotlib-v3.7.1-%23ffffff.svg?style=for-the-badge&logo=Matplotlib&logoColor=black)
![OpenCV](https://img.shields.io/badge/opencv-4.5.5-%23white.svg?style=for-the-badge&logo=opencv&logoColor=white)
![Colorama](https://img.shields.io/badge/colorama-v0.4.6-%23FFD700.svg?style=for-the-badge&logo=python&logoColor=white)
![Colorlog](https://img.shields.io/badge/colorlog-v6.7.0-%23FFD700.svg?style=for-the-badge&logo=python&logoColor=white)
![Pillow](https://img.shields.io/badge/Pillow-v9.3.0-%23FFD700.svg?style=for-the-badge&logo=python&logoColor=white)
![Seaborn](https://img.shields.io/badge/Seaborn-v0.12.2-%2355A3B2.svg?style=for-the-badge&logo=python&logoColor=white)


# ü©π Pill Metrics Learning with Multihead Attention

In object recognition, especially, when new classes can easily appear during the application, few-shot learning has 
great importance. Metrics learning is an important elementary technique for few-shot object recognition which can be 
applied successfully for pill recognition. To enforce the exploitation of different object features we use 
multi-stream metrics learning networks for pill recognition in our article. We investigate the usage of 
multihead attention layers at different parts of the network. The performance is analyzed on two datasets with 
superior results to a state-of-the-art multi-stream pill recognition network.

## üìê Architecture of the model

The main idea behind multi-stream processing is to persuade the sub-networks to focus on different kinds of features. For this reason different pre-processing steps are done in the streams:

-  **RGB**: color images are directly fed to a CNN for metrics embedding. We evaluated both EfficientNet-B0 [2] and 
EfficientNetv2 S [4]. EfficientNet-B0 has significantly fewer parameters than the CNN of [1] 
(5.3 million vs. 9 million) and it is well-optimized for similar tasks. EfficientNetv2 is larger (21.4M parameters) 
but is reported to be more accurate in ImageNet tasks and faster in train time. The same networks were used in all 
streams but with smaller number of parameters due to their grayscale input images.
-  **Contour**: images are generated by running the Canny edge detector on smoothed grayscale version of images 
(applying a 7√ó7 Gaussian kernel).
-  **Texture**: images are created by subtracting the smoothed and grayscale versions of pill images.
-  **Local Binary Patterns (LBP)**: LBP is a popular handcrafted local descriptor for many computer vision tasks 
including handwritten or printed OCR. That is the reason why we omitted the special OCR stream of [1] but computed the 
LBP images of the grayscale inputs and used them in similar streams as the others.

<figure align="center">
  <figcaption>Phase 1</figcaption>
  <img src="poc_images/substreams.png" alt="phase_1" width="1000"/>
</figure>

All streams received the bounding box defined pill images of resolution 224√ó224 detected by YOLOv7 as described above.
Before the concatenation of the embedding vectors we implemented the attention encoder in each stream. 
To fuse the information of the streams we concatenated the output vectors and applied full connections in one hidden 
and one output layer to generate the final embedding. During the training of the fusion network streams were frozen 
and only the top layers were trained.

<figure align="center">
  <figcaption>Phase 2</figcaption>
  <img src="poc_images/overview.png" alt="phase_2" width="1000"/>
</figure>


The following configurations were evaluated:

-  **EffNetV2+SA**: EfficientNetV2 S and separated self-attention.
-  **EffNetV2+MHA**: EfficientNetV2 S and separated multihead attention.
-  **EffNetV2+MHA+FMHA**: EfficientNetV2 S, separated multihead attention and multihead attention in the fusion network.

This repository only implements EffNetV2+SA, EffNetV2+MHA and EffNetV2+MHA+FMHA. EffNetV1+SA can be found on the 
`IDAACS2023` branch.


## üìä Datasets
We used two datasets, namely:
* CURE [1], that is available  [online](https://drive.google.com/drive/folders/1dcqUaTSepplc4GAUC05mr9iReWVqaThN).
* Our custom-made one, entitled OGYEIV1 [3]. It can be accessed if you contact me via my e-mail address:[ radli.richard@mik.uni-pannon.hu]()

The comparison of the two datasets can be seen in the table below:

<table>
    <thead>
    <tr style="background-color: #00000f">
      <th></th>
      <th>CURE</th>
      <th>OGYEIV1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Number of pill classes</td>
      <td>196</td>
      <td>78</td>
    </tr>
    <tr>
      <td>Number of images</td>
      <td>8973</td>
      <td>3154</td>
    </tr>
    <tr>
      <td>Image resolution</td>
      <td>800√ó800 - 2448√ó2448</td>
      <td>2465√ó1683</td>
    </tr>
    <tr>
      <td>Instance per class</td>
      <td>40-50</td>
      <td>40-60</td>
    </tr>
    <tr>
      <td>Segmentation labels</td>
      <td>no</td>
      <td>fully</td>
    </tr>
    <tr>
      <td>Backgrounds</td>
      <td>6</td>
      <td>1</td>
    </tr>
    <tr>
      <td>Imprinted text labels</td>
      <td>yes</td>
      <td>yes</td>
    </tr>
  </tbody>
</table>


## üìã Requirement
Make sure you have the following dependencies installed:

```bash
colorama>=0.4.6
colorlog>=6.7.0
json>=2.0.9
jsonschema>=4.23.0
matplotlib>=3.7.1
numpy>=1.23.5
opencv-python>=4.9.0
pandas>=2.1.0
Pillow>=9.3.0
seaborn>=0.12.2
scikit-image>=0.20.0
sklearn>=1.2.2
torch>=2.2.1+cu121
torchsummary>=1.5.1
torchvision>=0.17.1+cu121
tqdm>=4.65.0
```

Install dependencies with:
```bash
pip install -r requirements.txt
```

## üöÄ Installation

### 1. Clone or download the repository

Begin by cloning or downloading this repository to your local machine.

```bash
git clone https://github.com/richardRadli/pill_detection/tree/idaacs2023
```

### 2. Update configuration

Open the _data_paths.py_ file. You will find the following dictionary:

```python
root_mapping = {
    "your_username": {
        "STORAGE_ROOT ":
            "D:/storage/pill_detection/IDAACS23",
        "DATASET_ROOT":
            "D:/storage/pill_detection/IDAACS23/datasets",
        "PROJECT_ROOT":
            "C:/Users/ricsi/Documents/project/IVM",
    }
}
```

Replace "your_username" with your actual username. Run the following command in your terminal to confirm it:

```bash
whoami
```

Then update these paths:

#### STORAGE_ROOT:
* Adjust this path to the location where you want to save project outputs and other data generated during the execution 
of the Python files.

#### DATASET_ROOT:
* Modify this path to point to the directory where your datasets are stored. This folder should contain all datasets 
necessary for the project. It should look like this:

* D:\storage\pill_detection\IDAACS23\dataset
  * ogyei
    * Customer
    * Reference
    * etc.

#### PROJECT_ROOT:
* Update this path to the directory where the Python and JSON files of the project are located.

### 3. Create necessary folders
Run the data_paths.py script. This will create all the required folders based on the paths specified in the 
configuration.

### 4. Download and place datasets
Obtain the necessary datasets and place them into the DATASET_ROOT directory as specified in your updated configuration.

## üíª Usage

### Setting Up Configuration Files

Before running the Python scripts, you can configure your settings in the following JSON files:

* `augmentation_config.json` - Configuration for augmentation
* `fusion_net_config.json` - Configuration for training the fusion network
* `streamnet_config.json` - Configuration for training the stream networks
* `stream_images_config.json` - Configuration for image manipulation for the stream images

### Running the Pipeline
After updating paths and placing datasets, follow these steps to complete setup and training:
 
1. **Generate Augmented Images**
   * Run `augment_images.py` to create augmented image versions for training.

2. **Train YOLO network for object detection**
   * Train an arbitrary YOLO model with the augmented images for binary object detection. It should detect only the presence of a pill on the image.
 
3. **Generate Test Mask Images**
   * Do prediction on the test images with the trained YOLO network. Make sure to generate labels. After that use the
   `crop_yolo_detected_images.py` file to crop the detected bounding boxes.
 
4. **Generate Mask Images for Stream Training**
   * Run `draw_masks.py` to generate mask images needed for training stream networks.
 
5. **Create Stream Images** 
   * Generate RGB, contour, LBP, and texture stream images with `create_stream_images.py`.
   
6. **Split Images**
    * Split images by using the `copy_images_to_triplets.py` file. It will split images into _train/test_, 
   _anchor/pos_neg_, _ref/query_ folders.    

7. **Train Stream Networks** 
   * Train each of the four stream networks by running `train_stream_network.py`.
   
8. **Train Fusion Network**
   * After all stream networks are trained, use `train_fusion_network.py` to train the final fusion network.

9. **Evaluate Results** 
   * Run `predict_fusion_network.py` to evaluate the model and view the results.


## üì∞ Link to paper

For detailed insights, check out our [research paper](https://www.scitepress.org/Papers/2023/122355/122355.pdf).

## References
[1] - Ling, S., Pastor, A., Li, J., Che, Z., Wang, J., Kim, J., & Callet, P. L. (2020). Few-shot pill recognition. 
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 9789-9798).

[2] - Tan, M., & Le, Q. (2019, May). Efficientnet: Rethinking model scaling for convolutional neural networks. 
In International conference on machine learning (pp. 6105-6114). PMLR.

[3] - R√°dli, R.; V√∂r√∂sh√°zi, Z. and Cz√∫ni, L. (2023). Pill Metrics Learning with Multihead Attention.  
In Proceedings of the 15th International Joint Conference on Knowledge Discovery, Knowledge Engineering and
Knowledge Management - Volume 1: KDIR, ISBN 978-989-758-671-2, ISSN 2184-3228, pages 132-140.    

[4] - Tan, M., & Le, Q. (2021, July). Efficientnetv2: Smaller models and faster training. 
In International conference on machine learning (pp. 10096-10106). PMLR.
